<!--
**Av1352/Av1352** is a âœ¨ _special_ âœ¨ repository because its `README.md` (this file) appears on your GitHub profile.

Here are some ideas to get you started:

- ğŸ”­ Iâ€™m currently working on ...
- ğŸŒ± Iâ€™m currently learning ...
- ğŸ‘¯ Iâ€™m looking to collaborate on ...
- ğŸ¤” Iâ€™m looking for help with ...
- ğŸ’¬ Ask me about ...
- ğŸ“« How to reach me: ...
- ğŸ˜„ Pronouns: ...
- âš¡ Fun fact: ...
-->
<div align="center">

# Anju Vilashni Nandhakumar  
### Applied Machine Learning Engineer Â· Computer Vision Â· NLP Â· Reinforcement Learning

**MS in Artificial Intelligence, Northeastern University**  
Boston, MA Â· Open to Full-Time ML/AI Roles

I design and deploy ML systems where **correctness, interpretability, and real-world constraints**
matter more than leaderboard scores.

[![Portfolio](https://img.shields.io/badge/Portfolio-vxanju.com-73BA9B?style=for-the-badge&logo=google-chrome&logoColor=white)](https://vxanju.com)
[![LinkedIn](https://img.shields.io/badge/LinkedIn-anju--vilashni-0077B5?style=for-the-badge&logo=linkedin&logoColor=white)](https://linkedin.com/in/anju-vilashni)
[![Email](https://img.shields.io/badge/Email-nandhakumar.anju@gmail.com-DAB49D?style=for-the-badge&logo=gmail&logoColor=white)](mailto:nandhakumar.anju@gmail.com)

</div>

---

## ğŸ§  What I Build

- End-to-end ML systems from data â†’ model â†’ deployment  
- Explainable computer vision models for high-stakes domains  
- Transformer-based NLP systems on large-scale datasets  
- Reinforcement learning agents under safety and efficiency constraints  

My work focuses on **engineering tradeoffs**, not just model accuracy.

---

## ğŸ¯ Senior Highlights

- Reduced inference latency by **~20%** through GPU-aware optimization  
- Built **clinical-style ML pipelines** with explainability and reporting  
- Designed **multi-agent RL systems** with reward shaping and safety constraints  
- Deployed multiple models as **production-ready web systems**  

---

## ğŸ— Flagship Systems

### VisAIble â€” Explainable Deepfake Detection System  
**PyTorch Â· EfficientNet Â· Grad-CAM Â· LIME**

**Problem**  
Deepfake detection systems fail in real environments when predictions lack interpretability.

**Key Decisions**
- Chose EfficientNet-B0 over ViTs to meet latency constraints  
- Combined Grad-CAM and LIME for complementary explanations  
- Focused on false-positive reduction over raw accuracy

**Outcome**
- +8% accuracy over baseline CNN  
- ~22% reduction in false positives on unseen data  

ğŸ”— [Live Demo](https://visaible.streamlit.app/) Â· [Code](https://github.com/Av1352/VisAIble)

---

### Explainable Tumor Classification  
**TensorFlow Â· CNNs Â· SHAP Â· Medical Imaging**

**Problem**  
Black-box cancer classifiers are unsuitable for clinical decision support.

**Approach**
- CNN-based histopathology classification  
- SHAP + Grad-CAM overlays for clinician-interpretable outputs  

**Outcome**
- High-confidence predictions with visual explanation overlays  
- Designed for clinical review rather than benchmark optimization  

ğŸ”— [Live Demo](https://tumor-classification-xai.streamlit.app/) Â· [Code](https://github.com/Av1352/Tumor-Classification)

---

### Logical Fallacy Detection  
**ELECTRA Â· Transformers Â· NLP**

**Problem**  
Detecting nuanced logical fallacies requires reasoning beyond keyword matching.

**Approach**
- Fine-tuned ELECTRA on multi-class fallacy datasets  
- Combined contextual embeddings with case-based reasoning

**Outcome**
- Robust performance across subtle fallacy classes  
- Generalized better than classical classifiers on out-of-distribution samples  

ğŸ”— [Live Demo](https://logical-fallacy-detection.streamlit.app/) Â· [Code](https://github.com/Av1352/Logical-Fallacy-Detection)

---

### Autonomous Highway Reinforcement Learning  
**Rainbow DQN Â· A3C Â· Decision Transformers**

**Problem**  
RL agents trained in simulation often fail under safety-critical constraints.

**Key Learnings**
- Reward shaping mattered more than algorithm choice  
- Stable policies required conservative exploration strategies  

**Outcome**
- Achieved max reward of 48.2 in highway-env  
- Improved policy stability across random seeds  

ğŸ”— [Live Demo](https://highway-reinforecement-problem.streamlit.app/) Â· [Code](https://github.com/Av1352/Highway-RL)

---

## âš™ï¸ Engineering Notes (Things That Actually Mattered)

- Dataset leakage caused larger performance drops than architecture choice  
- Explainability methods were unstable across seeds â†’ fixed via normalization  
- Inference optimization had more real-world impact than marginal accuracy gains  
- RL agent behavior was dominated by reward design, not model complexity  

---

## ğŸ§° Tools I Reach For

**Modeling:** PyTorch, TensorFlow, Hugging Face  
**Explainability:** Grad-CAM, SHAP, LIME  
**Deployment:** Docker, AWS, Streamlit, Flask  
**Experimentation:** Scikit-learn, NumPy, Pandas  

---

## ğŸ“Š GitHub Activity

<div align="center">

<img
  src="https://github-readme-stats.vercel.app/api?username=Av1352&show_icons=true&count_private=true"
  alt="GitHub Stats"
  height="165"
/>

<img
  src="https://github-readme-streak-stats.herokuapp.com?user=Av1352"
  alt="GitHub Streak"
  height="165"
/>

</div>

---

## ğŸ“ Education

**Northeastern University** â€” MS in Artificial Intelligence  
**SRM Institute of Science and Technology** â€” BE Computer Science (AI/ML)

---

## ğŸ“ Publication

**Music Recommendation via Facial Emotion Recognition**  
*International Journal of Research and Analytical Reviews (IJRAR), 2022*  
ğŸ“„ https://ijrar.org/viewfull.php?&p_id=IJRAR22D2280

---

## ğŸ” Currently Exploring

- Explainability under distribution shift  
- Human-AI collaboration in safety-critical ML  
- Bridging academic ML and production constraints  

---

<div align="center">

*I build ML systems meant to survive outside notebooks.*  
â­ If something here helps you, feel free to star the repo.

</div>
